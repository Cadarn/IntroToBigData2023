{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1926eeba-1a2f-4742-a9a8-f63a09304f13",
   "metadata": {},
   "source": [
    "# Exploring SQL with DuckDB\n",
    "\n",
    "We can use DuckDB to run an in memory SQL database which can be used for a variety of use cases. Install it in Conda with:\n",
    "\n",
    "```\n",
    "conda install python-duckdb\n",
    "```\n",
    "\n",
    "Will will also want\n",
    "\n",
    "```\n",
    "conda install sqlalchemy ipython-sql duckdb-engine\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9a8f76-b766-4954-bc99-7383ed3e9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8367640b-7f6d-4094-9616-03c90f25cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import pymongo\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced0ba60-e5d3-4b69-8d1b-f6db5e63e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_path = Path(\"../../../datafiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b6e36-810c-446e-aadc-30050d2ce152",
   "metadata": {},
   "source": [
    "## Demo 1: Filtering large mixed light curve datasets\n",
    "I have some Swift space telescope light curve data in a cloud based implementation of Mongo DB. If you want to experiment there is a free-tier of [Mongo Atlas](https://www.mongodb.com/cloud/atlas/register) which you can register for if you want to experiment.\n",
    "\n",
    "There is a local snapshot of the data in the top-level datafiles directory: `datafiles/swift-lc/lightcurve_data.json`. You can load the data directly from the file to follow along; as an extention exercise you could try creating your own Mongo Atlas DB and uploading the data there, or you could use Docker to run a local Mongo database and use that akin to what we did in the Day 1 demo.\n",
    "\n",
    "### ASIDE: Securing credentials when sharing code\n",
    "The challenge is that I have security and I need a secure way to share this code without exposing my username, password etc; `python-dotenv` library to the rescue. This little library allows you to pull environment variables out of files, to be secure I make them hidden files and make sure to add them to my `.gitignore` file.\n",
    "\n",
    "So I locally have a `.env` file that has the following lines in it:\n",
    "```\n",
    "MONGO_USER = <username>\n",
    "MONGO_PASS = <password>\n",
    "MONGO_URI = <url_path_to_database>\n",
    "MONGO_CONNECTION_STRING = mongodb+srv://${MONGO_USER}:${MONGO_PASS}@${MONGO_URI}/?retryWrites=true&w=majority\n",
    "```\n",
    "\n",
    "This is a really good way to share code without exposing sensitive data; you can also load the file in as actual environment variables which is very useful when doing local development on code that is actually deployed locally. Check out the [python-dotenv documentation](https://pypi.org/project/python-dotenv/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd00a58-8484-457c-a182-46170d0de25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['MONGO_USER', 'MONGO_PASS', 'MONGO_URI', 'MONGO_CONNECTION_STRING'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "\n",
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ff2a10-b2ef-4743-bf20-9f49b97b91a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'api-access'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get(\"MONGO_USER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bbb5c-1bae-4164-b601-530fc13d02fd",
   "metadata": {},
   "source": [
    "## Connecting to Mongo\n",
    "\n",
    "Now that we have loaded the connection details I can establish a connection to the Mongo server, connect to the \"SwiftLC\" database and access the collection of \"lightcurves\" records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723813d2-a6a0-409d-b92b-c6e9a96e8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(config.get(\"MONGO_CONNECTION_STRING\"))\n",
    "\n",
    "db = client[\"SwiftLC\"]\n",
    "\n",
    "collection = db[\"lightcurves\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f2bbd-c2f0-49ab-ab36-ed85d2d3b838",
   "metadata": {},
   "source": [
    "If I return a single record we can see the form of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8495fea0-4746-446a-a09b-684a509e7649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DATETIME': datetime.datetime(2005, 2, 13, 0, 0),\n",
       " 'SOURCE': 'IGR J06074+2205',\n",
       " 'DATA_FLAG': 0,\n",
       " 'DAY': 44,\n",
       " 'TIMEDEL_EXPO': 832.0,\n",
       " 'TIMEDEL_DITHERED': 832.0,\n",
       " 'RATE': 0.00155646,\n",
       " 'TIMEDEL_CODED': 812.5,\n",
       " 'ERROR': 0.0018947,\n",
       " 'SYS_ERR': 5.448e-05,\n",
       " '_id': ObjectId('642880cb8dac64a4cfe06aa8'),\n",
       " 'YEAR': 2005,\n",
       " 'STAT_ERR': 0.0016215,\n",
       " 'TIME': 53414}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06cd982-2389-4015-b8c3-ff46ea0bb5cd",
   "metadata": {},
   "source": [
    "We will build a local dataframe of a sample of 200,000 light curve datapoints from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc9a61-9320-4ad6-977f-b78652c3fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_df = pd.DataFrame(collection.find({}, {\"_id\": 0}).limit(200_000))\n",
    "lightcurve_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c9491-82c8-4644-b318-32822329405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if you are loading the local copy of the data directly from file\n",
    "\n",
    "#import json\n",
    "#with open(local_data_path/\"swift-lc\"/\"lightcurve_data.json\", \"r\") as fileIn:\n",
    "#    lightcurve_df = pd.DataFrame(json.load(fileIn))\n",
    "#lightcurve_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea577c5-f82b-49c0-b030-debcd53257a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_df.SOURCE.unique()[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cafb234-e276-4a59-b523-dbe793068722",
   "metadata": {},
   "source": [
    "As an initial test let's see how long it takes to find all datapoints that have an exposure time of less than 200 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84b65c-975f-4a7b-93dc-17146376e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_time = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f28aef-547a-4a78-9267-b1d1da0c4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "test_result = lightcurve_df.query(\"TIMEDEL_EXPO < @exposure_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d83ca-ffb0-40f9-875a-1fc188f1cb6a",
   "metadata": {},
   "source": [
    "## Introducing DuckDB\n",
    "\n",
    "By passing the dataframe into DuckDB - an in-memory SQL database can we improve our calculation time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc5291-b37b-4776-83ef-e79b2cfd2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbcon = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f2e25-0ec9-46d4-b2be-72e62a3fc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbcon.execute(\"SELECT 1, 2, 3\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853919d9-ed27-407b-b2c2-ba2783dc4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbrun(query: str) -> pd.DataFrame:\n",
    "    \"\"\"Run the query with duckdb\"\"\"\n",
    "    result = dbcon.query(query).to_df()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197467c6-b06c-417d-8f73-19b95fb32d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM lightcurve_df WHERE TIMEDEL_EXPO < {exposure_time}\"\n",
    "exposure_under_200s_df = dbrun(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512b601-b150-4550-afa2-971f297012c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "exposure_under_200s_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b9195-5dfe-437c-ad4e-1bde5776033e",
   "metadata": {},
   "source": [
    "We can see that we get a massive speedup - over 100,000 times faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd2671-2bbc-44c5-9302-a51f51317424",
   "metadata": {},
   "source": [
    "## Exercise 1: Using DuckDB to complete our word count exercise\n",
    "\n",
    "As a reminder there are a few stages that we will have to follow to perform word counts across all of our books.\n",
    "\n",
    "### 1. Process a book into tokens\n",
    "We will first need a function that does the following:\n",
    "1. Read the data from a book file.\n",
    "2. Split all the words into individual tokens.\n",
    "3. Strip out any non-alphanumeric characters.\n",
    "4. Remove any \"blank\" tokens\n",
    "\n",
    "Write a function to do this, we've helped you out by providing a function that strips the non-alpha characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187c124-392e-487e-aa22-28a2f0378eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(s: str) -> str:\n",
    "    \"\"\"Strip removes any non-alpha charcters\"\"\"\n",
    "    return ''.join(filter(str.isalpha, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba8762-370b-461f-b612-a03c8ce14138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_book(file_path: Path):\n",
    "    with open(file_path, \"r\") as fileIn:\n",
    "        data = fileIn.read()\n",
    "    tokens = data.split()\n",
    "    stripped = map(strip, tokens)\n",
    "    notempty = filter(lambda w: len(w)>0, stripped)\n",
    "    return notempty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e28926-955e-4540-94d9-ec60aed70258",
   "metadata": {},
   "source": [
    "### 2. Looping over all of our book files anc creating a \"master\" list of words\n",
    "\n",
    "We have provided you with the root-folder where the book data is stored.\n",
    "\n",
    "Write a piece of code that finds all of the books (`.txt`) files in the given folder, extract all the individual tokens and put them into a master list of words called `words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd3f89-857c-4a8f-8612-2cf3384f9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_root = local_data_path/\"books/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b4f34-e9b7-4278-abd8-52ed88702041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here ...\n",
    "\n",
    "book_files = books_root.glob(\"*.txt\")\n",
    "\n",
    "words = []\n",
    "for book in book_files:\n",
    "    words += list(read_book(book))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfa6c6-36a5-442c-aeb5-da5f32a0bd44",
   "metadata": {},
   "source": [
    "We can now turn this into a dataframe and see how long it would take to count all the occurences of each unique word using the built in dataframe `.value_counts` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6b728-b274-4be2-84b4-9dff0f755177",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(words, columns=[\"word\"])\n",
    "\n",
    "print(f\"There are {len(words_df)} words in all our books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb689f2-f425-4683-ad68-db699e4218ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "res2 = words_df.value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd90ff-3c83-4b62-8d9b-d43c12b55dea",
   "metadata": {},
   "source": [
    "We now have a baseline for how long it could take to perform this task.\n",
    "\n",
    "### 3. Leveraging DuckDB & SQL to count all the words in the dataframe\n",
    "\n",
    "The final part of the exercise is to write the SQL query tht can count all of occurences of each unique word in the dataframe. Bonus points for sorting the words from most frequent to least frequent... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fb2c7-a95d-4361-86bd-dc2dffbf13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your query here ...\n",
    "wordcount_query = f\"SELECT word, COUNT(word) AS COUNT FROM words_df GROUP BY word ORDER BY COUNT DESC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb46b3-f833-4ffa-8d24-3faf61999021",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount_df = dbrun(wordcount_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2933223-8d2c-4266-a476-9f0e77f4b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "wordcount_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c54b6b-ba01-4876-8a2d-a2b068808673",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958a654-15a1-4d1a-8219-14225687e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60300d85-6d0b-4d02-80ff-34f42fad2acc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can see that we again get another speedup although this time it is only a factor of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38e8ba-7d44-4be7-bb83-7dc37c0e79cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ibd23]",
   "language": "python",
   "name": "conda-env-ibd23-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
